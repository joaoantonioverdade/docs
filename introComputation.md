## Theory of computation to revive my university days knowledge 

Resume of Sipser - Introduction to the theory of computation

-

There are three traditionally central areas of the theory of computation: 

* **automata**, deals with the definitions and properties of mathematical models of computation.
* **computability**, certain basic problems cannot be solved by computers. As for example the determination of a mathematical statement has true or false. The theory objective is to classify problems as solvable or not.
* **complexity**, *What makes some problems computationally hard and others easy?* we don't know the answer to it, but researchers have discovered an elegant scheme for classifying problems according to their computational difficulty.

-


### Chapter 1 - Regular Languages

A computational model is a idealized computer, accurate in some ways but not in others.

The simplest model is the **finite state machine** or **finite automata**

A finite automata is a model with finite states and inputs. For example a automatic door with two states: open and closed, with two inputs (some pads in the rear and front of the door) that reflect four possible inputs: front, read, both, none. Depending on the input the model jumps from one state to another: 


![State diagram](https://raw.githubusercontent.com/joaoantonioverdade/docs/master/resources/State_diagram_for_automatic_door.png)


The probabilistic counterpart are the **markov chains**.

A language is called a **regular language** if some finite automaton recognizes it.

In the theory of computation the objects are languages and the tools include operations specifically designed for manipulating them. The **regular operations** are three of them (A={good, bad}; B={boy, girl}):

* Union A U B, {good, bad, boy, girl}
* Concatenation A o B, {goodboy, goodgirl, badboy, badgirl}
* Star A*, {E, good, bad, goodgood, goodbad, badgood, goodbadbad, ...}


In a **nondeterministic** machine after a given input we don't know in which will be the next state.  

* DFA, deterministic finite automaton
* NFA, nondeterministic finite automaton (when the same input has two possible states, both *branches* are followed:

![NFA example](https://raw.githubusercontent.com/joaoantonioverdade/docs/master/resources/NFA%20example.png)

Every nondeterministic finite automaton has an equivalent deterministic finite automaton.

**Regular expressions** are build up expressions describing languages, originated from regular operations, ex.: (0 U 1)*

Regular expressions and finite automata are equivalent in their descriptive power. Any regular expression can be converted into a finite automaton that recognizes the language it describes and vice versa.

Certain languages cannot be recognized by any finite automaton, ex.: L={0^n 1^n | n >=0 }, a finite automaton needs a finite number of states, which in the language L isn't limited.

Common examples of non-regular languages are the HTML or XML, where regular expressions fail to parse them.


### Chapter 2 - Context-free Languages

**Context-free grammars** are a a more powerful method than regular expressions or finite automata for describing languages, allowing a recursive structure.

Example of grammar:

A -> 0A1

A -> B

B -> #


Sometimes a grammar can generate the same string in several different ways. Such a string will have several different parse trees and thus several different meanings. If a grammar generates the same string in several different ways, we say that the string is derived **ambiguously** in that grammar.


When working with context-free grammars, it is often convenient to have them in simplified form. One of the simplest and most useful forms is called the Chomsky normal form. Chomsky normal form is useful in giving algorithms for working with context-free grammars. Any context-free language is generated by a context-free grammar in Chomsky normal form.


**Pushdown automata** (PDA) is a computational model like nondeterministic finite automat but with an extra component called a stack. A stack provides additional memory beyond the finite amount available in the control, allowing to recognize some nonregular languages. They are equivalent in power to the context-free grammars.

A stack can hold an unlimited amount of information. For the non regular language L=0^n 1^n | n>=0 the following informal description shows how the pushdown automaton works:

Read symbols from the input. As each 0 is read, push it onto the stack. As soon as 1s are seen, pop a 0 off the stack for each 1 read. If reading the input is finished exactly when the stack becomes empty of 0s, accpet the input. If the stack becomes empty while 1s remain or if the 1s are finished while the stack still contains 0s or if any 0s appear in the input following 1s, reject the input.


### Chapter 3 - Turing Machines

Similar to a finite automaton but with an unlimited and unrestricted memory, a **Turing machine** is a much accurate model of a general purpose computer. A Turing machine can do everything that a real computer can do. Nonetheless, even a Turing machine cannot solve certain problems. In a very real sense, these problems are beyond the theoretical limits of computation.


The Turing machine model uses an infinite tape as its unlimited memory. It has a tape head that can read and write symbols and move around on the tape. Initially the tape contains only the input string and is blank everywhere eles. If the machine needs to store information, it may write this information on the tape. To read the information that it has written, the machine can move its head back over it. The machine continues computing until it decides to produce an output. The outputs *accept* and *reject* are obtained by entering designated accepting and rejecting states. If it doesn't enter an accepting or a rejecting state, it will go on forever, never halting.

The differences between finite automata and Turing machines:
* A turing machine can both write on the tape and read from it
* The read-write head can move both to the left and to the right
* The tape is infinite
* The special states for rejecting and accepting take effect immediately

When we start a Turing machine on an input, three outcomes are possible. The machine may accept, reject or loop. By loop we mean that the machine simply does not halt. Looping may entail any simple or complex behavior that never leads to a halting state.

Distinguishing a machine that is looping from one that is merely taking a long time is difficult. For this reason we prefer Turing machines that halt on all inputs; such machines never loop. These machine are called **deciders** because they always make a decision to accept or reject. 

If a Turing machine can decide a language this is called Turing-decidable, decidable or recursive language.

Informally speaking, an **algorithm** is a collection of simple instructions for carrying out some task. The definition came in the 1936 papers of Alonzo Church and Alan Turing. Church used a notational system called the lambda-calculus to define algorithms. Turing did it with his *machines*. These two definitions were shown to be equivalent. This connection between the informal notion of algorithm and the precise definition has come to be called the **Church-Turing thesis**.

### Chapter 4 - Decidability

Why study unsolvability? Knowing when a problem is algorithmically unsolvable is useful because then you realize that the problem must be simplified or altered before you can find an algorithmic solution. Like any tool, computers have capabilities and limitations that must be appreciated if they are to be used well. 

Every context-free language is decidable (given a finite sequence of symbols as input, accepts it if belongs to the language and rejects it otherwise).

![Relationship among classes of languages](https://raw.githubusercontent.com/joaoantonioverdade/docs/master/resources/relationship_among_classes_of_languages.png)


One of the most philosophically important theorems of the theory of computation is the **halting problem**.
There is a specific problem that is algorithmically unsolvable like verifying if a program performs as specified. There can be no general procedure to decide if a self-contained computer program will eventually halt. The halting problem is undecidable.


### Chapter 5 - Reducibility

The primary method for proving thatproblems are computationally unsolvable is the **reducibility**.
A reduction is a way of converting one problem to another problem in such a way that a solution to the second problem can be used to solve the first problem.

Reducibility always involves two problems, which we call A and B. If A reduces to B, we can use a solution to B to solve A. So in our example, A is the problem of finding your way around the city and B is the problem of obtaining a map. Note that reducibility says nothing about solving A or B alone, but only about the solvability of A in the presence of a solution to B.

When A is reducible to B, solving A cannot be harder that solving B because a solution to B gives a solution to A. In terms of computability theory, if A is reducible to B and B is decidable, A also is decidable. Equivalently, if A is undecidable and reducible to B, B is undecidable.


### Chapter 6 - Advanced topics in computability theory

Making machines that reproduce themselves is possible. The recursion theorem demonstrates how. The recursion theorem states that Turing machines can obtain their own description and then go on to compute with it. 

Instead of a single, universal definition of information, several definitions are used - depending upon the application. Next will be presented the computability theory of information. Starting with the following binary sequences:

A = 010101010101010101010101010101010

B = 111001011010001111010100000011101

Intuitively, sequence A contains little information because it is merely a repetition of the pattern 01 n times. In contrast, sequence B appears to contain more information. We define the quantity of information contained in an object to be the size of that object's smallest representation or description. By a description of an object we mean a precise and unambigous characterization of the object so that we may recreate it from the description alone. Thus sequence A contains little information because it has a small description, whereas sequence B apparently contains more information because it seems to have no concise description.


### Chapter 7 - Time Complexity


Measuring complexity, taking the language A = { 0^k 1^k | k >= 0}. Obviously A is a decidable language. How much time does a single-tape Turing machine need to decide A? The turing machine:

M1 = On input string w: 
* Scan across the tape and reject if a 0 is found to the right of a 1.
* Repeat if both 0s and 1s remain on the tape:
 * Scan across the tape, crossing off a single 0 and a single 1.
* If 0s still remain after all the 1s have been crossed off, or if 1s still remain after all the 0s have been crossed off, reject. Otherwise, if neither 0s nor 1s remain on the tape, accept. 

The number of steps that an algorithm uses on a particular input may depende on several parameters. The input, nodes, edges... For simplicity we compute the running time of an algorithm purely as a function of the length of the string representing the input and don't consider any other parameters. 

Because the exact running time of an algorithm often is a complex expression, we usually just estimate it using a convenient form of estimation called **asymptotic analysis**. The running time of the algorithm is used with large inputs, with the highest order term of the expression and diregarding both the coefficient of that term and any lower order terms. For example the function f(n) = 6n³ + 2n² + 20n +45 has four terms, with 6n³ with the highest order. Disregarding the coefficient 6, we say that f is asymptotically at most n³. The **asymptotic notation** or **big-O notation** for describing this relationship is f(n) = O(n³). 

Analyzing the algorithm M1:

* Each of its four stages are seen separately.
* In stage 1 the machine scans across the tape, performing this scan uses n steps. Repositioning the head at the left-hand end of the tape uses another n steps. Total: 2n steps. In big-O notation we say that this stage uses O(n) steps.
* In stages 2 and 3 the machine repeatedly scans the tape and crosses off a 0 and 1 on each scan. Each scan uses O(n) steps. Because each scan crosses of two symbols, at most n/2 scans can occur. Total: (n/2)O(n) = O(n²) steps.
* In stage 4 the machine makes a single scan. Total: O(n)

Total time on an input of length n is O(n) + O(n²) + O(n), or O(n²).




